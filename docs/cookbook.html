<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cookbook - simple_onnx</title>
    <link rel="stylesheet" href="css/style.css">
</head>
<body>
    <header>
        <h1>Cookbook</h1>
        <p class="tagline">Real-world examples and recipes</p>
    </header>

    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="quick.html">Quick API</a></li>
            <li><a href="user-guide.html">User Guide</a></li>
            <li><a href="architecture.html">Architecture</a></li>
        </ul>
    </nav>

    <main>
        <section id="basic-inference">
            <h2>Recipe 1: Basic Image Classification</h2>
            <p>Run ResNet50 on an image and get top-5 predictions.</p>
            <pre><code>classify_image (image_file: STRING): ARRAYED_LIST [TUPLE [label: STRING; confidence: REAL]] is
    local
        onnx: SIMPLE_ONNX
        model: ONNX_MODEL
        session: ONNX_SESSION
        tensor: ONNX_TENSOR
        result: ONNX_RESULT
        predictions: ARRAYED_LIST [TUPLE [label: STRING; confidence: REAL]]
        raw_output: ARRAY [REAL_32]
        i, top_idx: INTEGER
        max_conf: REAL
    do
        create onnx.make

        -- Load model
        model := onnx.load_model ("resnet50.onnx")

        -- Create session
        session := onnx.create_session (model)
        session.set_optimization_level (2)
        session.load

        -- Prepare input (image as tensor [1, 3, 224, 224])
        tensor := onnx.create_tensor_float32 (
            onnx.create_shape (<< 1, 3, 224, 224 >>)
        )

        -- Load image and normalize (your image loading code)
        tensor.set_data_from_array (load_and_normalize_image (image_file))

        -- Run inference
        result := session.execute (tensor)

        create predictions.make
        if result.is_success then
            raw_output := result.output_tensor.to_array
            -- Extract top 5 predictions (pseudocode for brevity)
            -- In practice: sort by confidence, get labels from mapping file
        end

        session.unload
        Result := predictions
    end</code></pre>
        </section>

        <section id="batch-processing">
            <h2>Recipe 2: Batch Processing Multiple Inputs</h2>
            <p>Process multiple images in a batch for efficiency.</p>
            <pre><code>batch_classify (image_files: LIST [STRING]): ARRAYED_LIST [ARRAYED_LIST [STRING]] is
    local
        onnx: SIMPLE_ONNX
        model: ONNX_MODEL
        session: ONNX_SESSION
        batch_size: INTEGER
        batch_tensor: ONNX_TENSOR
        results_list: ARRAYED_LIST [ARRAYED_LIST [STRING]]
        i, batch_idx: INTEGER
    do
        batch_size := 32
        create onnx.make
        model := onnx.load_model ("resnet50.onnx")
        session := onnx.create_session (model)
        session.set_provider ("CUDAExecutionProvider")  -- Use GPU
        session.load

        create results_list.make

        -- Process in batches
        from i := 1 until i > image_files.count loop
            -- Create batch tensor [batch_size, 3, 224, 224]
            batch_tensor := onnx.create_tensor_float32 (
                onnx.create_shape (<< batch_size, 3, 224, 224 >>)
            )

            -- Load images for this batch
            load_batch_data (batch_tensor, image_files, i, batch_size)

            -- Run inference
            if attached session.execute (batch_tensor) as result then
                if result.is_success then
                    results_list.force (extract_predictions (result))
                end
            end

            i := i + batch_size
        end

        session.unload
        Result := results_list
    end</code></pre>
        </section>

        <section id="error-handling-recipe">
            <h2>Recipe 3: Robust Error Handling</h2>
            <p>Gracefully handle errors during inference.</p>
            <pre><code>safe_inference (model_path: STRING; input_data: ARRAY [REAL_32]): detachable ARRAY [REAL_32] is
    local
        onnx: SIMPLE_ONNX
        model: ONNX_MODEL
        session: ONNX_SESSION
        tensor: ONNX_TENSOR
        result: ONNX_RESULT
        error_log: LOG_FILE
    do
        create error_log.create ("inference_errors.log")

        -- Validate input
        if input_data = Void or else input_data.count = 0 then
            error_log.log ("Error: Input data is empty or void")
            return
        end

        -- Try to initialize
        create onnx.make
        model := onnx.load_model (model_path)
        if model = Void then
            error_log.log ("Error: Could not load model from " + model_path)
            return
        end

        -- Try to create session
        session := onnx.create_session (model)
        if session = Void then
            error_log.log ("Error: Could not create session")
            return
        end

        -- Check provider availability
        if not onnx.is_provider_available ("CUDAExecutionProvider") then
            error_log.log ("Warning: GPU not available, using CPU")
            session.set_provider ("CPUExecutionProvider")
        else
            session.set_provider ("CUDAExecutionProvider")
        end

        session.load

        -- Create and run inference
        tensor := onnx.create_tensor_float32 (
            onnx.create_shape (<< 1, input_data.count >>)
        )
        tensor.set_data_from_array (input_data)

        result := session.execute (tensor)

        -- Handle result
        if result.is_success then
            Result := result.output_tensor.to_array
        else
            error_log.log ("Inference failed: " + result.error_message)
            error_log.log ("Error code: " + result.error_code.out)
        end

        session.unload
        error_log.close
    end</code></pre>
        </section>

        <section id="provider-selection">
            <h2>Recipe 4: Smart Provider Selection</h2>
            <p>Automatically select the best available execution provider.</p>
            <pre><code>select_best_provider (session: ONNX_SESSION): STRING is
    local
        onnx: SIMPLE_ONNX
        providers: ARRAYED_LIST [STRING]
        preferred: ARRAY [STRING]
        i, j: INTEGER
        provider_name: STRING
    do
        create onnx.make

        -- Order of preference: TensorRT > CUDA > CoreML > CPU
        preferred := << "TensorrtExecutionProvider", "CUDAExecutionProvider",
                       "CoreMLExecutionProvider", "CPUExecutionProvider" >>

        providers := onnx.available_providers

        from i := 1 until i > preferred.count loop
            provider_name := preferred [i]
            if providers.has (provider_name) then
                session.set_provider (provider_name)
                Result := provider_name
                return
            end
            i := i + 1
        end

        -- Fallback to CPU
        session.set_provider ("CPUExecutionProvider")
        Result := "CPUExecutionProvider"
    end</code></pre>
        </section>

        <section id="multi-input-output">
            <h2>Recipe 5: Multi-Input/Output Model</h2>
            <p>Handle models with multiple inputs and outputs.</p>
            <pre><code>multi_input_inference (model: ONNX_MODEL): ARRAYED_LIST [ONNX_TENSOR] is
    local
        onnx: SIMPLE_ONNX
        session: ONNX_SESSION
        input1, input2: ONNX_TENSOR
        result: ONNX_RESULT
        outputs: ARRAYED_LIST [ONNX_TENSOR]
    do
        create onnx.make
        session := onnx.create_session (model)
        session.load

        -- Create first input [1, 256, 256, 3] float32
        input1 := onnx.create_tensor_float32 (
            onnx.create_shape (<< 1, 256, 256, 3 >>)
        )
        input1.set_data_from_array (load_image_data ("image1.jpg"))

        -- Create second input [1, 10] int32
        input2 := onnx.create_tensor_int32 (
            onnx.create_shape (<< 1, 10 >>)
        )
        input2.set_int_data_from_array (load_metadata ("metadata.txt"))

        -- Note: This is simplified. In practice, you'd need to handle
        -- multiple inputs by modifying execute() or batching them differently
        -- For now, showing single execution

        result := session.execute (input1)

        create outputs.make
        if result.is_success then
            outputs.force (result.output_tensor)
        end

        session.unload
        Result := outputs
    end</code></pre>
        </section>

        <section id="performance-tuning">
            <h2>Recipe 6: Performance Tuning</h2>
            <p>Optimize for latency-sensitive applications.</p>
            <pre><code>create_optimized_session (model: ONNX_MODEL): ONNX_SESSION is
    local
        onnx: SIMPLE_ONNX
        session: ONNX_SESSION
        providers: ARRAYED_LIST [STRING]
    do
        create onnx.make
        session := onnx.create_session (model)

        -- Strategy 1: Use GPU if available
        providers := onnx.available_providers
        if providers.has ("CUDAExecutionProvider") then
            session.set_provider ("CUDAExecutionProvider")
        end

        -- Strategy 2: Maximum graph optimization
        session.set_optimization_level (3)  -- All optimizations

        -- Strategy 3: Load into memory
        session.load

        Result := session
    end</code></pre>
        </section>

        <section id="monitoring">
            <h2>Recipe 7: Monitoring & Logging</h2>
            <p>Track inference metrics for production monitoring.</p>
            <pre><code>monitored_inference (session: ONNX_SESSION; input: ONNX_TENSOR): TUPLE [output: ONNX_TENSOR; elapsed_ms: REAL] is
    local
        result: ONNX_RESULT
        start_time, end_time: REAL
        elapsed: REAL
    do
        start_time := current_time_ms

        result := session.execute (input)

        end_time := current_time_ms
        elapsed := end_time - start_time

        -- Log metrics
        if result.is_success then
            log_inference_success (elapsed, result.output_tensor)
            Result := [result.output_tensor, elapsed]
        else
            log_inference_failure (result.error_message, elapsed)
        end
    end

    current_time_ms: REAL is
        -- Platform-dependent: Get current time in milliseconds
        do
            -- Implementation: Use platform timer
        end

    log_inference_success (elapsed_ms: REAL; output: ONNX_TENSOR) is
        do
            -- Write to metrics system
            -- Example: output.shape.element_count, elapsed_ms
        end

    log_inference_failure (error: STRING; elapsed_ms: REAL) is
        do
            -- Log error and timing
        end</code></pre>
        </section>

        <section id="cleanup">
            <h2>Recipe 8: Clean Shutdown</h2>
            <p>Properly clean up resources in your application.</p>
            <pre><code>main is
    local
        onnx: SIMPLE_ONNX
        model: ONNX_MODEL
        session: ONNX_SESSION
        result: ONNX_RESULT
    do
        create onnx.make

        -- Use across block to ensure cleanup
        if attached model_path as mp then
            model := onnx.load_model (mp)
            if model /= Void then
                session := onnx.create_session (model)
                if session /= Void then
                    session.load
                    -- ... run inference ...
                    session.unload  -- IMPORTANT: Release C resources
                end
            end
        end
    end</code></pre>
        </section>

        <footer>
            <p><a href="user-guide.html">→ See User Guide for concepts and patterns</a></p>
            <p><a href="api-reference.html">→ See API Reference for complete documentation</a></p>
        </footer>
    </main>
</body>
</html>
